{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2200500744663790822\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4153868288\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 18307743582942485810\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "def load_az_dataset(datasetPath):\n",
    "\t# initialize the list of data and labels\n",
    "\tdata = []\n",
    "\tlabels = []\n",
    "\t# loop over the rows of the A-Z handwritten digit dataset\n",
    "\tfor row in open(datasetPath):\n",
    "\t\t# parse the label and image from the row\n",
    "\t\trow = row.split(\",\")\n",
    "\t\tlabel = int(row[0])\n",
    "\t\timage = np.array([int(x) for x in row[1:]], dtype=\"uint8\")\n",
    "\t\t# images are represented as single channel (grayscale) images\n",
    "\t\t# that are 28x28=784 pixels -- we need to take this flattened\n",
    "\t\t# 784-d list of numbers and repshape them into a 28x28 matrix\n",
    "\t\timage = image.reshape((28, 28))\n",
    "\t\t# update the list of data and labels\n",
    "\t\tdata.append(image)\n",
    "\t\tlabels.append(label)       \n",
    "\t# convert the data and labels to NumPy arrays\n",
    "\tdata = np.array(data, dtype=\"float32\")\n",
    "\tlabels = np.array(labels, dtype=\"int\")\n",
    "\t# return a 2-tuple of the A-Z data and labels\n",
    "\treturn (data, labels)\n",
    "\n",
    "def load_mnist_dataset():\n",
    "\t# load the MNIST dataset and stack the training data and testing\n",
    "\t# data together (we'll create our own training and testing splits\n",
    "\t# later in the project)\n",
    "\t((trainData, trainLabels), (testData, testLabels)) = mnist.load_data()\n",
    "\tdata = np.vstack([trainData, testData])\n",
    "\tlabels = np.hstack([trainLabels, testLabels])\n",
    "\t# return a 2-tuple of the MNIST data and labels\n",
    "\treturn (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class ResNet:\n",
    "\t@staticmethod\n",
    "\tdef residual_module(data, K, stride, chanDim, red=False,\n",
    "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
    "\t\t# the shortcut branch of the ResNet module should be\n",
    "\t\t# initialize as the input (identity) data\n",
    "\t\tshortcut = data\n",
    "\n",
    "\t\t# the first block of the ResNet module are the 1x1 CONVs\n",
    "\t\tbn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(data)\n",
    "\t\tact1 = Activation(\"relu\")(bn1)\n",
    "\t\tconv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "\t\t# the second block of the ResNet module are the 3x3 CONVs\n",
    "\t\tbn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(conv1)\n",
    "\t\tact2 = Activation(\"relu\")(bn2)\n",
    "\t\tconv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n",
    "\t\t\tpadding=\"same\", use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act2)\n",
    "\n",
    "\t\t# the third block of the ResNet module is another set of 1x1\n",
    "\t\t# CONVs\n",
    "\t\tbn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(conv2)\n",
    "\t\tact3 = Activation(\"relu\")(bn3)\n",
    "\t\tconv3 = Conv2D(K, (1, 1), use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act3)\n",
    "\n",
    "\t\t# if we are to reduce the spatial size, apply a CONV layer to\n",
    "\t\t# the shortcut\n",
    "\t\tif red:\n",
    "\t\t\tshortcut = Conv2D(K, (1, 1), strides=stride,\n",
    "\t\t\t\tuse_bias=False, kernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "\t\t# add together the shortcut and the final CONV\n",
    "\t\tx = add([conv3, shortcut])\n",
    "\n",
    "\t\t# return the addition as the output of the ResNet module\n",
    "\t\treturn x\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, classes, stages, filters,\n",
    "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
    "\t\t# initialize the input shape to be \"channels last\" and the\n",
    "\t\t# channels dimension itself\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\t# if we are using \"channels first\", update the input shape\n",
    "\t\t# and channels dimension\n",
    "\t\tif K.image_data_format() == \"channels_first\":\n",
    "\t\t\tinputShape = (depth, height, width)\n",
    "\t\t\tchanDim = 1\n",
    "\n",
    "\t\t# set the input and then apply a BN followed by CONV\n",
    "\t\tinputs = Input(shape=inputShape)\n",
    "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(inputs)\n",
    "\t\tx = Conv2D(filters[0], (3, 3), use_bias=False,\n",
    "\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\n",
    "\t\t# loop over the number of stages\n",
    "\t\tfor i in range(0, len(stages)):\n",
    "\t\t\t# initialize the stride, then apply a residual module\n",
    "\t\t\t# used to reduce the spatial size of the input volume\n",
    "\t\t\tstride = (1, 1) if i == 0 else (2, 2)\n",
    "\t\t\tx = ResNet.residual_module(x, filters[i + 1], stride,\n",
    "\t\t\t\tchanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "\t\t\t# loop over the number of layers in the stage\n",
    "\t\t\tfor j in range(0, stages[i] - 1):\n",
    "\t\t\t\t# apply a ResNet module\n",
    "\t\t\t\tx = ResNet.residual_module(x, filters[i + 1],\n",
    "\t\t\t\t\t(1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "\t\t# apply BN => ACT => POOL\n",
    "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = AveragePooling2D((8, 8))(x)\n",
    "\n",
    "\t\t# softmax classifier\n",
    "\t\tx = Flatten()(x)\n",
    "\t\tx = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
    "\t\tx = Activation(\"softmax\")(x)\n",
    "\n",
    "\t\t# create the model\n",
    "\t\tmodel = Model(inputs, x, name=\"resnet\")\n",
    "\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading datasets...\n",
      "[INFO] compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexc\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "WARNING:tensorflow:From C:\\Users\\alexc\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Epoch 1/50\n",
      "2765/2765 [==============================] - 232s 82ms/step - loss: 1.8711 - accuracy: 0.8733 - val_loss: 0.4869 - val_accuracy: 0.9271\n",
      "Epoch 2/50\n",
      "2765/2765 [==============================] - 228s 83ms/step - loss: 0.9316 - accuracy: 0.9309 - val_loss: 0.5761 - val_accuracy: 0.8869\n",
      "Epoch 3/50\n",
      "2765/2765 [==============================] - 229s 83ms/step - loss: 0.8596 - accuracy: 0.9359 - val_loss: 0.5793 - val_accuracy: 0.8789\n",
      "Epoch 4/50\n",
      "2765/2765 [==============================] - 230s 83ms/step - loss: 0.8141 - accuracy: 0.9384 - val_loss: 0.6050 - val_accuracy: 0.8629\n",
      "Epoch 5/50\n",
      "2765/2765 [==============================] - 231s 83ms/step - loss: 0.7899 - accuracy: 0.9406 - val_loss: 0.5903 - val_accuracy: 0.8704\n",
      "Epoch 6/50\n",
      "2765/2765 [==============================] - 228s 83ms/step - loss: 0.7663 - accuracy: 0.9423 - val_loss: 0.5080 - val_accuracy: 0.9038\n",
      "Epoch 7/50\n",
      "2765/2765 [==============================] - 228s 82ms/step - loss: 0.7505 - accuracy: 0.9442 - val_loss: 0.5061 - val_accuracy: 0.9009\n",
      "Epoch 8/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.7401 - accuracy: 0.9448 - val_loss: 0.4632 - val_accuracy: 0.9254\n",
      "Epoch 9/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.7258 - accuracy: 0.9465 - val_loss: 0.4931 - val_accuracy: 0.9084\n",
      "Epoch 10/50\n",
      "2765/2765 [==============================] - 228s 82ms/step - loss: 0.7163 - accuracy: 0.9481 - val_loss: 0.4683 - val_accuracy: 0.9207\n",
      "Epoch 11/50\n",
      "2765/2765 [==============================] - 228s 82ms/step - loss: 0.7104 - accuracy: 0.9493 - val_loss: 0.4954 - val_accuracy: 0.9024\n",
      "Epoch 12/50\n",
      "2765/2765 [==============================] - 228s 82ms/step - loss: 0.6985 - accuracy: 0.9504 - val_loss: 0.4452 - val_accuracy: 0.9316\n",
      "Epoch 13/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.6913 - accuracy: 0.9514 - val_loss: 0.4546 - val_accuracy: 0.9266\n",
      "Epoch 14/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.6850 - accuracy: 0.9526 - val_loss: 0.4177 - val_accuracy: 0.9497\n",
      "Epoch 15/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.6778 - accuracy: 0.9535 - val_loss: 0.4360 - val_accuracy: 0.9376\n",
      "Epoch 16/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.6726 - accuracy: 0.9545 - val_loss: 0.4272 - val_accuracy: 0.9449\n",
      "Epoch 17/50\n",
      "2765/2765 [==============================] - 228s 82ms/step - loss: 0.6692 - accuracy: 0.9550 - val_loss: 0.4014 - val_accuracy: 0.9596\n",
      "Epoch 18/50\n",
      "2765/2765 [==============================] - 228s 82ms/step - loss: 0.6633 - accuracy: 0.9552 - val_loss: 0.4096 - val_accuracy: 0.9546\n",
      "Epoch 19/50\n",
      "2765/2765 [==============================] - 226s 82ms/step - loss: 0.6586 - accuracy: 0.9562 - val_loss: 0.4219 - val_accuracy: 0.9440\n",
      "Epoch 20/50\n",
      "2765/2765 [==============================] - 228s 82ms/step - loss: 0.6518 - accuracy: 0.9570 - val_loss: 0.4125 - val_accuracy: 0.9517\n",
      "Epoch 21/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.6488 - accuracy: 0.9577 - val_loss: 0.3950 - val_accuracy: 0.9621\n",
      "Epoch 22/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.6458 - accuracy: 0.9582 - val_loss: 0.3923 - val_accuracy: 0.9630\n",
      "Epoch 23/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.6432 - accuracy: 0.9581 - val_loss: 0.3910 - val_accuracy: 0.9626\n",
      "Epoch 24/50\n",
      "2765/2765 [==============================] - 228s 82ms/step - loss: 0.6403 - accuracy: 0.9587 - val_loss: 0.3815 - val_accuracy: 0.9662\n",
      "Epoch 25/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.6357 - accuracy: 0.9594 - val_loss: 0.3850 - val_accuracy: 0.9648\n",
      "Epoch 26/50\n",
      "2765/2765 [==============================] - 226s 82ms/step - loss: 0.6320 - accuracy: 0.9602 - val_loss: 0.3889 - val_accuracy: 0.9634\n",
      "Epoch 27/50\n",
      "2765/2765 [==============================] - 226s 82ms/step - loss: 0.6294 - accuracy: 0.9598 - val_loss: 0.3909 - val_accuracy: 0.9621\n",
      "Epoch 28/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.6276 - accuracy: 0.9601 - val_loss: 0.3985 - val_accuracy: 0.9584\n",
      "Epoch 29/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.6222 - accuracy: 0.9604 - val_loss: 0.3824 - val_accuracy: 0.9652\n",
      "Epoch 30/50\n",
      "2765/2765 [==============================] - 228s 82ms/step - loss: 0.6197 - accuracy: 0.9613 - val_loss: 0.3796 - val_accuracy: 0.9663\n",
      "Epoch 31/50\n",
      "2765/2765 [==============================] - 226s 82ms/step - loss: 0.6177 - accuracy: 0.9615 - val_loss: 0.3856 - val_accuracy: 0.9636\n",
      "Epoch 32/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.6191 - accuracy: 0.9612 - val_loss: 0.3830 - val_accuracy: 0.9649\n",
      "Epoch 33/50\n",
      "2765/2765 [==============================] - 228s 82ms/step - loss: 0.6119 - accuracy: 0.9623 - val_loss: 0.3845 - val_accuracy: 0.9642\n",
      "Epoch 34/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.6145 - accuracy: 0.9618 - val_loss: 0.3724 - val_accuracy: 0.9687\n",
      "Epoch 35/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.6097 - accuracy: 0.9624 - val_loss: 0.3720 - val_accuracy: 0.9693\n",
      "Epoch 36/50\n",
      "2765/2765 [==============================] - 226s 82ms/step - loss: 0.6079 - accuracy: 0.9629 - val_loss: 0.3753 - val_accuracy: 0.9672\n",
      "Epoch 37/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.6054 - accuracy: 0.9636 - val_loss: 0.3772 - val_accuracy: 0.9665\n",
      "Epoch 38/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.6020 - accuracy: 0.9633 - val_loss: 0.3741 - val_accuracy: 0.9678\n",
      "Epoch 39/50\n",
      "2765/2765 [==============================] - 226s 82ms/step - loss: 0.6017 - accuracy: 0.9636 - val_loss: 0.3686 - val_accuracy: 0.9696\n",
      "Epoch 40/50\n",
      "2765/2765 [==============================] - 224s 81ms/step - loss: 0.6007 - accuracy: 0.9640 - val_loss: 0.3712 - val_accuracy: 0.9685\n",
      "Epoch 41/50\n",
      "2765/2765 [==============================] - 225s 81ms/step - loss: 0.5965 - accuracy: 0.9640 - val_loss: 0.4008 - val_accuracy: 0.9535\n",
      "Epoch 42/50\n",
      "2765/2765 [==============================] - 226s 82ms/step - loss: 0.5953 - accuracy: 0.9647 - val_loss: 0.3801 - val_accuracy: 0.9645\n",
      "Epoch 43/50\n",
      "2765/2765 [==============================] - 225s 82ms/step - loss: 0.5941 - accuracy: 0.9646 - val_loss: 0.3681 - val_accuracy: 0.9688\n",
      "Epoch 44/50\n",
      "2765/2765 [==============================] - 225s 82ms/step - loss: 0.5948 - accuracy: 0.9647 - val_loss: 0.3721 - val_accuracy: 0.9674\n",
      "Epoch 45/50\n",
      "2765/2765 [==============================] - 226s 82ms/step - loss: 0.5919 - accuracy: 0.9651 - val_loss: 0.3636 - val_accuracy: 0.9702\n",
      "Epoch 46/50\n",
      "2765/2765 [==============================] - 227s 82ms/step - loss: 0.5864 - accuracy: 0.9657 - val_loss: 0.3742 - val_accuracy: 0.9671\n",
      "Epoch 47/50\n",
      "2765/2765 [==============================] - 226s 82ms/step - loss: 0.5885 - accuracy: 0.9656 - val_loss: 0.3660 - val_accuracy: 0.9697\n",
      "Epoch 48/50\n",
      "2765/2765 [==============================] - 226s 82ms/step - loss: 0.5895 - accuracy: 0.9653 - val_loss: 0.3660 - val_accuracy: 0.9695\n",
      "Epoch 49/50\n",
      "2765/2765 [==============================] - 226s 82ms/step - loss: 0.5859 - accuracy: 0.9657 - val_loss: 0.3592 - val_accuracy: 0.9722\n",
      "Epoch 50/50\n",
      "2765/2765 [==============================] - 226s 82ms/step - loss: 0.5835 - accuracy: 0.9664 - val_loss: 0.3598 - val_accuracy: 0.9717\n",
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.43      0.57      1381\n",
      "           1       0.99      0.99      0.99      1575\n",
      "           2       0.95      0.96      0.96      1398\n",
      "           3       0.98      0.99      0.99      1428\n",
      "           4       0.96      0.97      0.96      1365\n",
      "           5       0.75      0.94      0.84      1263\n",
      "           6       0.96      0.98      0.97      1375\n",
      "           7       0.97      0.99      0.98      1459\n",
      "           8       0.96      0.99      0.98      1365\n",
      "           9       0.99      0.98      0.99      1392\n",
      "           A       1.00      0.99      1.00      2774\n",
      "           B       0.99      0.99      0.99      1734\n",
      "           C       0.99      0.99      0.99      4682\n",
      "           D       0.92      0.98      0.95      2027\n",
      "           E       0.99      0.99      0.99      2288\n",
      "           F       0.98      0.99      0.98       232\n",
      "           G       0.99      0.95      0.97      1152\n",
      "           H       0.97      0.98      0.98      1444\n",
      "           I       0.98      0.99      0.98       224\n",
      "           J       0.97      0.97      0.97      1699\n",
      "           K       0.97      0.99      0.98      1121\n",
      "           L       0.98      0.99      0.99      2317\n",
      "           M       0.99      0.99      0.99      2467\n",
      "           N       0.99      0.99      0.99      3802\n",
      "           O       0.93      0.98      0.95     11565\n",
      "           P       1.00      0.99      0.99      3868\n",
      "           Q       0.97      0.98      0.98      1162\n",
      "           R       0.99      0.99      0.99      2313\n",
      "           S       0.99      0.95      0.97      9684\n",
      "           T       1.00      0.99      0.99      4499\n",
      "           U       0.99      0.99      0.99      5802\n",
      "           V       0.97      1.00      0.98       836\n",
      "           W       0.99      0.99      0.99      2157\n",
      "           X       0.98      1.00      0.99      1254\n",
      "           Y       0.98      0.97      0.98      2172\n",
      "           Z       0.95      0.96      0.95      1215\n",
      "\n",
      "    accuracy                           0.97     88491\n",
      "   macro avg       0.97      0.97      0.96     88491\n",
      "weighted avg       0.97      0.97      0.97     88491\n",
      "\n",
      "[INFO] serializing network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexc\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import build_montages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "# initialize the number of epochs to train for, initial learning rate,\n",
    "# and batch size\n",
    "EPOCHS = 50\n",
    "INIT_LR = 1e-1\n",
    "BS = 128\n",
    "# load the A-Z and MNIST datasets, respectively\n",
    "print(\"[INFO] loading datasets...\")\n",
    "(azData, azLabels) = load_az_dataset(r\"C:\\GitHub\\mf724-ocr\\az-dataset\\A_Z Handwritten Data.csv\")\n",
    "(digitsData, digitsLabels) = load_mnist_dataset()\n",
    "\n",
    "# the MNIST dataset occupies the labels 0-9, so let's add 10 to every\n",
    "# A-Z label to ensure the A-Z characters are not incorrectly labeled\n",
    "# as digits\n",
    "azLabels += 10\n",
    "# stack the A-Z data and labels with the MNIST digits data and labels\n",
    "data = np.vstack([azData, digitsData])\n",
    "labels = np.hstack([azLabels, digitsLabels])\n",
    "# each image in the A-Z and MNIST digts datasets are 28x28 pixels;\n",
    "# however, the architecture we're using is designed for 32x32 images,\n",
    "# so we need to resize them to 32x32\n",
    "data = [cv2.resize(image, (32, 32)) for image in data]\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "# add a channel dimension to every image in the dataset and scale the\n",
    "# pixel intensities of the images from [0, 255] down to [0, 1]\n",
    "data = np.expand_dims(data, axis=-1)\n",
    "data /= 255.0\n",
    "\n",
    "# convert the labels from integers to vectors\n",
    "le = LabelBinarizer()\n",
    "labels = le.fit_transform(labels)\n",
    "counts = labels.sum(axis=0)\n",
    "# account for skew in the labeled data\n",
    "classTotals = labels.sum(axis=0)\n",
    "classWeight = {}\n",
    "# loop over all classes and calculate the class weight\n",
    "for i in range(0, len(classTotals)):\n",
    "\tclassWeight[i] = classTotals.max() / classTotals[i]\n",
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data,\n",
    "\tlabels, test_size=0.20, stratify=labels, random_state=42)\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "\trotation_range=10,\n",
    "\tzoom_range=0.05,\n",
    "\twidth_shift_range=0.1,\n",
    "\theight_shift_range=0.1,\n",
    "\tshear_range=0.15,\n",
    "\thorizontal_flip=False,\n",
    "\tfill_mode=\"nearest\")\n",
    "\n",
    "# initialize and compile our deep neural network\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model = ResNet.build(32, 32, 1, len(le.classes_), (3, 3, 3),\n",
    "\t(64, 64, 128, 256), reg=0.0005)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit(\n",
    "\taug.flow(trainX, trainY, batch_size=BS),\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tsteps_per_epoch=len(trainX) // BS,\n",
    "\tepochs=EPOCHS,\n",
    "\tclass_weight=classWeight,\n",
    "\tverbose=1)\n",
    "# define the list of label names\n",
    "labelNames = \"0123456789\"\n",
    "labelNames += \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "labelNames = [l for l in labelNames]\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BS)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1), target_names=labelNames))\n",
    "\n",
    "# save the model to disk\n",
    "print(\"[INFO] serializing network...\")\n",
    "model.save(\"model.pkl\", save_format=\"h5\")\n",
    "# construct a plot that plots and saves the training history\n",
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"train_imgea.jpeg\")\n",
    "\n",
    "# initialize our list of output test images\n",
    "images = []\n",
    "# randomly select a few testing characters\n",
    "for i in np.random.choice(np.arange(0, len(testY)), size=(49,)):\n",
    "\t# classify the character\n",
    "\tprobs = model.predict(testX[np.newaxis, i])\n",
    "\tprediction = probs.argmax(axis=1)\n",
    "\tlabel = labelNames[prediction[0]]\n",
    "\t# extract the image from the test data and initialize the text\n",
    "\t# label color as green (correct)\n",
    "\timage = (testX[i] * 255).astype(\"uint8\")\n",
    "\tcolor = (0, 255, 0)\n",
    "\t# otherwise, the class label prediction is incorrect\n",
    "\tif prediction[0] != np.argmax(testY[i]):\n",
    "\t\tcolor = (0, 0, 255)\n",
    "\t# merge the channels into one image, resize the image from 32x32\n",
    "\t# to 96x96 so we can better see it and then draw the predicted\n",
    "\t# label on the image\n",
    "\timage = cv2.merge([image] * 3)\n",
    "\timage = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR)\n",
    "\tcv2.putText(image, label, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75,\n",
    "\t\tcolor, 2)\n",
    "\t# add the image to our list of output images\n",
    "\timages.append(image)\n",
    "# construct the montage for the images\n",
    "montage = build_montages(images, (96, 96), (7, 7))[0]\n",
    "# show the output montage\n",
    "cv2.imshow(\"OCR Results\", montage)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
